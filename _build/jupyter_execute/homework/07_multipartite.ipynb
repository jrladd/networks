{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82ef5b0-1196-4ff2-9b16-8b839a41ab5d",
   "metadata": {},
   "source": [
    "# Multipartite and Multilayer Networks\n",
    "\n",
    "This week we'll explore the two special kinds of networks we discussed in class: multipartite networks, which are for more than one kind of node, and multilayer networks, which are for more than one kind of edge.\n",
    "\n",
    "In a short Jupyter notebook report, complete the following tasks. Don't simply calculate the answers: make sure you're fully explaining (in writing) the metrics and visualizations that you generate. Consider the [Criteria for Good Reports](https://jrladd.com/CIS397-networks/criteria/) as a guide. You can create markdown cells with section headers to separate the different sections of the report. Rather than number the report as if you're answering distinct questions, use the questions as a guide to do some data storytelling, i.e. explain this network's data in an organized way.\n",
    "\n",
    "1. Download an [Affiliation network](http://konect.cc/categories/Affiliation/) of your choice from the KONECT project datasets. (If you're unable to unzip these files, download one of the ones available under Sakai Resources.) Load the data into NetworkX and calculate at least 3 kinds of *bipartite* centrality on this network. Export the data to a `.gml` file with those measures included. Visualize your bipartite network in either Gephi or NetworkX, using one of your centrality measures as the size of the nodes. What do you learn about the nodes in your network with high centrality?\n",
    "\n",
    "> *n.b. Loading this data will require some adjustment of the original data that you downloaded. Refer to our discussion of this from class, and reach out if you have any questions.*\n",
    "\n",
    "2. Using the same dataset, project your network in a *unipartite* graph, onto the nodeset of *people* in your network. Visualize this new network in either Gephi or NetworkX. What information is gained by presenting the network in this way, and how does it help you understand the people in your network? What information is lost?\n",
    "3. Download the `quaker_edgelist.csv` data from the textbook datasets page. This is a historical dynamic/temporal network that shows changing relationships over time. Using [this tutorial](https://seinecle.github.io/gephi-tutorials/generated-html/converting-a-network-with-dates-into-dynamic.html), create a timeline for this network in Gephi and create a visualization showing changes in the network over time. Include two or three screenshots showing key moments in the network's history (include the timeline at the bottom of the Gephi window in your screenshots). During what period in time were most of the relationships in this network active? How do you know this?\n",
    "4. NetworkX has no functionality for multilayer networks. To work with dynamic and temporal networks, we'll try out the [Teneto](https://teneto.readthedocs.io/en/latest/tutorial/networkrepresentation.html) library. This week you'll read the docs and try to work it out yourself, but later I'll post some basic functions in the the textbook for reference. Load the `quaker_edgelist.csv` as a dataframe and put it into a Teneto `TemporalNetwork` object. I've included some code here for converting the edgelist into the appropriate form for Teneto, but give it a try yourself first. (You may also need `quaker_nodelist.csv` for this.) Once you have a temporal network object, calculate temporal degree, betweenness, and closeness centrality. Also calculate topological overlap. Who are the most important people in this network over time, and how consistent is the network over time?\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the Teneto dataprep solution</summary>\n",
    "<pre><code>nodes = pd.read_csv(\"quaker_nodelist.csv\")\n",
    "all_nodes = nodes.name.to_list()\n",
    "\n",
    "intervals = range(q[\"Start Year\"].min(),q[\"End Year\"].max(),20)\n",
    "new_edges = []\n",
    "for n,row in q.iterrows():\n",
    "    for i,x in enumerate(intervals):\n",
    "        try:\n",
    "            if row[\"Start Year\"] <= x <= row[\"End Year\"]:\n",
    "                new_edges.append({'i': all_nodes.index(row['Source']), 'j': all_nodes.index(row['Target']), 't': i})\n",
    "        except IndexError:\n",
    "            if row[\"Start Year\"] <= x <= q[\"End Year\"].max():\n",
    "                new_edges.append({'i': row['Source ID'], 'j': row['Target ID'], 't': i})\n",
    "\n",
    "new_df = pd.DataFrame(new_edges)\n",
    "new_df</code></pre>\n",
    "</details>\n",
    "\n",
    "\n",
    "**When you're finished, remove these instructions from the top of the file, leaving only your own writing and code. Export the notebook as an HTML file, check to make sure everything is formatted correctly, and submit your HTML file to Sakai.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225dbf9d-4fa1-40aa-bc54-48824a51c4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}